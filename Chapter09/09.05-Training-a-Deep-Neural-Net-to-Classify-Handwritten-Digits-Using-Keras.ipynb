{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Deep Neural Net to Classify Handwritten Digits Using Keras\n",
    "\n",
    "Although we achieved a formidable score with the MLP above, our result does not hold up\n",
    "to state-of-the-art results. Currently, the best result has close to 99.8% accuracyâ€”better than\n",
    "human performance! This is why nowadays, the task of classifying handwritten digits is\n",
    "largely regarded as solved.\n",
    "\n",
    "To get closer to the state-of-the-art results, we need to use state-of-the-art techniques. Thus,\n",
    "we return to Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the MNIST dataset\n",
    "\n",
    "To make sure we get the same result every time we run the experiment, we will pick a\n",
    "random seed for NumPy's random number generator. This way, shuffling the training\n",
    "samples from the MNIST dataset will always result in the same order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T06:48:50.550029200Z",
     "start_time": "2023-07-25T06:48:50.479241400Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a loading function similar to train_test_split from scikit-learn's\n",
    "`model_selection` module. Its syntax might look strangely familiar to you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T07:11:38.158870100Z",
     "start_time": "2023-07-25T07:11:02.292550400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 2us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 29s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 5s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# (X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural nets in Keras act on the feature matrix slightly differently than the standard\n",
    "OpenCV and scikit-learn estimators. Whereas the rows of a feature matrix in Keras still\n",
    "correspond to the number of samples (`X_train.shape[0]` in the code below), we can\n",
    "preserve the two-dimensional nature of the input images by adding more dimensions to the\n",
    "feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have reshaped the feature matrix into a four-dimensional matrix with dimensions\n",
    "`n_features x 28 x 28 x 1`.\n",
    "We also made sure we operate on 32-bit floating\n",
    "point numbers between [0, 1], rather than unsigned integers in [0, 255]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can one-hot encode the training labels like we did before. This will make sure each\n",
    "category of target labels can be assigned to a neuron in the output layer. We could do this\n",
    "with scikit-learn's `preprocessing`, but in this case it is easier to use Keras' own utility\n",
    "function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "n_classes = 10\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a convolutional neural network\n",
    "\n",
    "Once we have preprocessed the data, it is time to define the actual model. Here, we will\n",
    "once again rely on the `Sequential` model to define a feedforward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this time, we will be smarter about the individual layers. We will design our\n",
    "neural network around a **convolutional layer**, where the kernel is a 3 x 3 pixel two-dimensional\n",
    "convolution.\n",
    "\n",
    "A two-dimensional convolutional layer operates akin to image filtering in\n",
    "OpenCV, where each image in the input data is convolved with a small\n",
    "two-dimensional kernel. In Keras, we can specify the kernel size and the\n",
    "stride:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "n_filters = 32\n",
    "kernel_size = (3, 3)\n",
    "model.add(Conv2D(n_filters, (kernel_size[0], kernel_size[1]),\n",
    "                 padding='valid',\n",
    "                 input_shape=input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we will use a linear rectified unit as an activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a deep convolutional neural net, we can have as many layers as we want. A popular\n",
    "version of this structure applied to MNIST involves performing the convolution and\n",
    "rectification twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(n_filters, (kernel_size[0], kernel_size[1])))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will pool the activations and add a `Dropout` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D, Dropout\n",
    "pool_size = (2, 2)\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will flatten the model and finally pass it through a `softmax` function to arrive at\n",
    "the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Dense\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use the cross-entropy loss and the Adadelta algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualize the model's summary which will list all the layers along with their respective dimensions and the number of weights each layer consists. It will also provide you information about the total number of parameters in your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 600,810\n",
      "Trainable params: 600,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are in total 600,810 parameters that will be trained and will require good amount of computation power!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "We fit the model like we do with all other classifiers:\n",
    "\n",
    "> Caution! This might take several hours depending on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 64s - loss: 0.3953 - acc: 0.8793 - val_loss: 0.0986 - val_acc: 0.9694\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 62s - loss: 0.1379 - acc: 0.9594 - val_loss: 0.0597 - val_acc: 0.9806\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 62s - loss: 0.1022 - acc: 0.9700 - val_loss: 0.0507 - val_acc: 0.9842\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.0859 - acc: 0.9748 - val_loss: 0.0450 - val_acc: 0.9857\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.0762 - acc: 0.9763 - val_loss: 0.0380 - val_acc: 0.9875\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.0680 - acc: 0.9797 - val_loss: 0.0373 - val_acc: 0.9882\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.0610 - acc: 0.9820 - val_loss: 0.0367 - val_acc: 0.9865\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 62s - loss: 0.0587 - acc: 0.9829 - val_loss: 0.0365 - val_acc: 0.9884\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.0526 - acc: 0.9839 - val_loss: 0.0325 - val_acc: 0.9893\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 62s - loss: 0.0510 - acc: 0.9847 - val_loss: 0.0323 - val_acc: 0.9888\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.0495 - acc: 0.9850 - val_loss: 0.0308 - val_acc: 0.9900\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 65s - loss: 0.0470 - acc: 0.9862 - val_loss: 0.0302 - val_acc: 0.9900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x138d2e208>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=128, epochs=12,\n",
    "          verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training completes, we can evaluate the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9856/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03018923045709962, 0.99]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we achieved 99% accuracy! Worlds apart from the MLP classifier we implemented\n",
    "before. And this is just one way to do things. As you can see, neural networks provide a\n",
    "plethora of tuning parameters, and it is not at all clear which ones will lead to the best\n",
    "performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
