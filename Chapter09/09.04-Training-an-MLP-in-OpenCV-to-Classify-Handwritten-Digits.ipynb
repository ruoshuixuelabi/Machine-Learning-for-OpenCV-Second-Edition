{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reference: https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an MLP in OpenCV to Classify Handwritten Digits\n",
    "\n",
    "In this section, we will use an MLP in OpenCV to classify\n",
    "handwritten digits from the popular MNIST dataset, which has been constructed by Yann\n",
    "LeCun and colleagues and serves as a popular benchmark dataset for machine learning\n",
    "algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset\n",
    "\n",
    "The easiest way to obtain the MNIST dataset is using Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T06:45:05.355562600Z",
     "start_time": "2023-07-25T06:45:05.283291700Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.src' has no attribute 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# from keras.datasets import mnist\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# from tensorflow.python.keras.utils.np_utils import to_categorical\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\n\u001B[0;32m      6\u001B[0m (x_train, y_train), (x_test, y_test) \u001B[38;5;241m=\u001B[39m keras\u001B[38;5;241m.\u001B[39mdatasets\u001B[38;5;241m.\u001B[39mmnist\u001B[38;5;241m.\u001B[39mload_data()\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\u001B[39;00m\n",
      "File \u001B[1;32mD:\\python\\lib\\site-packages\\keras\\__init__.py:5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __internal__\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m activations\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m applications\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m callbacks\n",
      "File \u001B[1;32mD:\\python\\lib\\site-packages\\keras\\applications\\__init__.py:8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m efficientnet_v2\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m imagenet_utils\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m inception_resnet_v2\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m inception_v3\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mobilenet\n",
      "File \u001B[1;32mD:\\python\\lib\\site-packages\\keras\\applications\\inception_resnet_v2\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minception_resnet_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InceptionResNetV2\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minception_resnet_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m decode_predictions\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minception_resnet_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m preprocess_input\n",
      "File \u001B[1;32mD:\\python\\lib\\site-packages\\keras\\src\\applications\\__init__.py:41\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mefficientnet_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EfficientNetV2M\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mefficientnet_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EfficientNetV2S\n\u001B[1;32m---> 41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minception_resnet_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InceptionResNetV2\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minception_v3\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InceptionV3\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmobilenet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MobileNet\n",
      "File \u001B[1;32mD:\\python\\lib\\site-packages\\keras\\src\\applications\\inception_resnet_v2.py:324\u001B[0m\n\u001B[0;32m    320\u001B[0m         x \u001B[38;5;241m=\u001B[39m layers\u001B[38;5;241m.\u001B[39mActivation(activation, name\u001B[38;5;241m=\u001B[39mac_name)(x)\n\u001B[0;32m    321\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;129m@keras\u001B[39m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241m.\u001B[39mregister_keras_serializable()\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mCustomScaleLayer\u001B[39;00m(keras_layers\u001B[38;5;241m.\u001B[39mLayer):\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, scale, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'keras.src' has no attribute 'utils'"
     ]
    }
   ],
   "source": [
    "# from keras.datasets import mnist\n",
    "# from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will download the data from the Amazon Cloud (might take a while depending on\n",
    "your internet connection) and automatically split the data into training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data comes in a format that we are already familiar with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should take note that the labels come as integer values between zero and nine\n",
    "(corresponding to the digits 0-9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at some example digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_train[i, :, :], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.savefig('mnist-examples.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the MNIST dataset is the successor to the NIST digits dataset provided by scikit-learn\n",
    "that we used before (`sklearn.datasets.load_digits` (refer to [Chapter 2](02.00-Working-with-Data-in-OpenCV.ipynb), *Working\n",
    "with Data in OpenCV and Python*).\n",
    "\n",
    "Some notable differences are as follows:\n",
    "- MNIST images are significantly larger (28x28 pixels) than NIST images (8x8 pixels), thus paying more attention to fine details, such as distortions and individual differences between images of the same digit\n",
    "- The MNIST dataset is much larger than the NIST dataset, providing 60,000 training and 10,000 test samples (as compared to a total of 5,620 NIST images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the MNIST dataset\n",
    "\n",
    "As we learned in [Chapter 4](04.00-Representing-Data-and-Engineering-Features.ipynb), *Representing Data and Engineering Features*, there are a number\n",
    "of preprocessing steps we might like to apply here, such as **centering**, **scaling**, and **representing categorical features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to transform `y_train` and `y_test` is by the one-hot encoder from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(sparse=False, dtype=np.float32, categories='auto')\n",
    "y_train_pre = enc.fit_transform(y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will transform the labels of the training set from a `<n_samples x 1>` vector with\n",
    "integers 0-9 into a `<n_samples x 10>` matrix with floating point numbers 0.0 or 1.0.\n",
    "\n",
    "Analogously, we can transform `y_test` using the same procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pre = enc.fit_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we need to preprocess `X_train` and `X_test` for the purpose of working with\n",
    "OpenCV. Currently, `X_train` and `X_test` are 3-D matrices `<n_samples x 28 x 28>`\n",
    "with integer values between 0 and 255. Preferably, we want a 2-D matrix `<n_samples x\n",
    "n_features>` with floating point numbers, where `n_features` is 784, basically flatten the 28 x 28 images to 784-dimensional vector::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre = X_train.astype(np.float32) / 255.0\n",
    "X_train_pre = X_train_pre.reshape((X_train.shape[0], -1))\n",
    "X_test_pre = X_test.astype(np.float32) / 255.0\n",
    "X_test_pre = X_test_pre.reshape((X_test.shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are ready to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an MLP using OpenCV\n",
    "\n",
    "We can set up and train an MLP in OpenCV with the following recipe:\n",
    "\n",
    "Instantiate a new MLP object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "mlp = cv2.ml.ANN_MLP_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the size of every layer in the network. We are free to add as many layers as we want, but we need to make sure that the first layer has the same number of neurons as input features (784 in our case), and that the last layer has the same number of neurons as class labels (10 in our case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.setLayerSizes(np.array([784, 512, 512, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify an activation function. Here we use the sigmoidal activation function\n",
    "from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.setActivationFunction(cv2.ml.ANN_MLP_SIGMOID_SYM, 2.5, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the training method. Here we use the backpropagation algorithm\n",
    "described above. We also need to make sure that we choose a small enough\n",
    "learning rate. Since we have on the order of $10^5$ training samples, it is a good idea\n",
    "to set the learning rate to at most $10^{-5}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.setTrainMethod(cv2.ml.ANN_MLP_BACKPROP)\n",
    "mlp.setBackpropWeightScale(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the termination criteria. Here we use the same criteria as above: to run\n",
    "training for ten iterations (`term_max_iter`) or until the error does no longer\n",
    "decrease significantly (`term_eps`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_mode = cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS\n",
    "term_max_iter = 10\n",
    "term_eps = 0.01\n",
    "mlp.setTermCriteria((term_mode, term_max_iter, term_eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.setTrainMethod(cv2.ml.ANN_MLP_BACKPROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network on the training set (`X_train_pre`):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the training completes, we can calculate the accuracy score on the training set to see\n",
    "how far we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.train(X_train_pre, cv2.ml.ROW_SAMPLE, y_train_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_hat_train = mlp.predict(X_train_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_hat_train.round(), y_train_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, of course, what really counts is the accuracy score we get on the held-out test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_hat_test = mlp.predict(X_test_pre)\n",
    "accuracy_score(y_hat_test.round(), y_test_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "91.7% accuracy is not bad at all if you ask me! The first thing you should try is to change the\n",
    "layer sizes in `In [10]` above, and see how the test score changes.\n",
    "As you add more neurons\n",
    "to the network, you should see the training score increase—and with it, hopefully, the test\n",
    "score.\n",
    "\n",
    "However, having `N` neurons in a single layer is not the same as having them spread\n",
    "out over several layers! Can you confirm this observation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
